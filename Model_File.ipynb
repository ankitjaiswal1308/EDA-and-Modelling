{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Default_Prediction:\n",
    "    def __init__(self):\n",
    "        self.discard_col = [\n",
    "            'decision_id',\n",
    "            'label',\n",
    "            'device_name',\n",
    "            'gms_version',\n",
    "            'brand',\n",
    "            'carrier',\n",
    "            'manufacturer',\n",
    "            'screen_height', \n",
    "            'v11', \n",
    "            'v12', \n",
    "            'v19', \n",
    "            'v2', \n",
    "            'v21', \n",
    "            'v35', \n",
    "            'v6',\n",
    "        ]\n",
    "        self.cat_col = [\n",
    "            'screen_dpi',\n",
    "            'network_type'\n",
    "        ]\n",
    "        \n",
    "        self.num_col = []\n",
    "        \n",
    "    def remove_null_row(self, null_count_threshold = 40):\n",
    "        null_row_index = list(self.df[self.df.isna().sum(axis=1)>=null_count_threshold].index)\n",
    "        self.df.drop(null_row_index, axis=0, inplace=True)\n",
    "    \n",
    "    def add_null_flag_column(self, null_flag_cols = ['v27','v1','v44','v45']):\n",
    "        for col in null_flag_cols:\n",
    "            self.df[col+'_nflag'] = self.df[col].apply(lambda x: np.where(np.isnan(x), 1, 0))\n",
    "    \n",
    "    def data_impute(self, impute_dict = {}):\n",
    "        self.impute_dict = impute_dict\n",
    "        if(len(list(self.impute_dict.keys()))==0):\n",
    "            for col in self.df.columns:\n",
    "                if col in self.cat_col:\n",
    "                    self.impute_dict[col] = self.df[col].mode()[0]\n",
    "                else:\n",
    "                    self.impute_dict[col] = self.df[col].median()\n",
    "        for col in self.df.columns:\n",
    "            impute_val = self.impute_dict[col]\n",
    "            self.df[col].fillna(impute_val, inplace = True)\n",
    "\n",
    "    \n",
    "    def remove_outliers(self):\n",
    "        outlier_indices = []\n",
    "        self.num_col = list(set(self.df.columns)-set(self.cat_col))\n",
    "        #iterate over columns of features\n",
    "        for c in self.num_col:\n",
    "            # 1st quartile\n",
    "            Q1 = np.percentile(self.df[c],25)\n",
    "            # 3rd quartile\n",
    "            Q3 = np.percentile(self.df[c],75)\n",
    "            # IQR\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Outlier step\n",
    "            outlier_step = IQR * 1.5\n",
    "            # detect outlier and their indeces( a list of indeces of outliers for a feature columns)   \n",
    "            outlier_list_col = self.df[(self.df[c] < Q1 - outlier_step) | (self.df[c] > Q3 + outlier_step)].index\n",
    "            # store indeces (append the outlier indices that we found  for column to the list of outlier indices )\n",
    "            outlier_indices.extend(outlier_list_col)\n",
    "        # select observation containing more than outliers\n",
    "        outlier_indices = Counter(outlier_indices)\n",
    "        multiple_outliers = list(i for i, v in outlier_indices.items() if v > 20)\n",
    "        self.df.drop(multiple_outliers, axis = 0, inplace = True)\n",
    "    \n",
    "\n",
    "    def get_data_encoding(self, code_run_mode = 'Train'):\n",
    "        self.df = pd.get_dummies(self.df, columns=self.cat_col, dummy_na=False)\n",
    "        if(code_run_mode == 'Train'):\n",
    "            self.model_cols = list(self.df.columns)\n",
    "            # save self.model_cols\n",
    "        else:\n",
    "            #some values in categorical data may not be present in test/prediction data\n",
    "            enc_cols = list(self.df.columns)\n",
    "            for col in set(self.model_cols)-set(enc_cols):\n",
    "                self.df[col] = 0\n",
    "            \n",
    "            # some new class/values may be present in test/prediction data\n",
    "            self.df.drop(list(set(enc_cols)-set(self.model_cols)), axis = 1, inplace = True)\n",
    "    \n",
    "    def data_preprocessing(self, code_run_mode = 'Train'):\n",
    "        print('0 data preprocessing starts', self.df.shape)\n",
    "        \n",
    "        self.df.drop(self.discard_col, axis = 1, inplace = True)\n",
    "        print(\"1 dropping unwanted column\", self.df.shape)       \n",
    "        \n",
    "        self.add_null_flag_column()\n",
    "        print(\"2 add null columns\", self.df.shape)\n",
    "        \n",
    "        for col in self.cat_col:\n",
    "            self.df[col] = self.df[col].str.lower()\n",
    "        print(\"3 lower-case cat columns\", self.df.shape)\n",
    "        \n",
    "        if(code_run_mode == 'Train'):\n",
    "            self.remove_null_row()\n",
    "            print(\"4 dropping null rows\", self.df.shape)\n",
    "            \n",
    "            self.data_impute()\n",
    "            print(\"5 data_imputation\", self.df.shape)\n",
    "            #save impute_dict\n",
    "            self.remove_outliers()\n",
    "            print(\"6 remove outliers\", self.df.shape)\n",
    "            \n",
    "        else:\n",
    "            self.data_impute(self.impute_dict)\n",
    "            print(\"5 data imputation\", self.df.shape)\n",
    "            \n",
    "        self.get_data_encoding(code_run_mode)\n",
    "        print(\"7 data_encoding\", self.df.shape)\n",
    "        \n",
    "    def get_model_hyper_param(self):\n",
    "        param = {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "        return param\n",
    "    \n",
    "    def save_model_artifact(self):\n",
    "        model_artifact = {}\n",
    "        model_artifact['impute_dict'] = self.impute_dict\n",
    "        model_artifact['model_cols'] = list(self.model_cols)\n",
    "        #model_artifact['model_param'] = self.model_param\n",
    "        with open(\"model_artifact.json\", \"w\") as outfile:\n",
    "            json.dump(model_artifact, outfile)\n",
    "\n",
    "    def load_model_artifact(self):\n",
    "        with open('model_artifact.json', 'r') as openfile:\n",
    "            model_artifact = json.load(openfile)\n",
    "        \n",
    "        self.impute_dict = model_artifact['impute_dict']\n",
    "        self.model_cols = model_artifact['model_cols']\n",
    "        #self.model_param = model_artifact('model_param')\n",
    "\n",
    "        \n",
    "    def model_training(self, df):\n",
    "        self.df = df\n",
    "        print('0 model trainaing begins', self.df.shape)\n",
    "        self.data_preprocessing(code_run_mode = 'Train')\n",
    "        \n",
    "        X = self.df.drop(['default'], axis = 1)\n",
    "        y = self.df['default']\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        param = self.get_model_hyper_param()\n",
    "        model.set_params(**param)\n",
    "        \n",
    "        \n",
    "        feature_scale = MinMaxScaler()\n",
    "        self.pipeline = Pipeline(steps = [('feature-scaling', feature_scale), ('model', model)])\n",
    "        self.pipeline = self.pipeline.fit(X, y)\n",
    "        \n",
    "        self.save_model_artifact()\n",
    "        pickle.dump(self.pipeline, open('model.sav', 'wb'))\n",
    "        \n",
    "        \n",
    "    def model_prediction(self, df):\n",
    "        self.df = df\n",
    "        self.load_model_artifact()\n",
    "        self.data_preprocessing(code_run_mode = 'Test')\n",
    "        self.pipeline = pickle.load(open('model.sav', 'rb'))\n",
    "        \n",
    "        X_pred = self.df[self.model_cols].drop('default', axis = 1)\n",
    "        y_pred = self.pipeline.predict(X_pred)\n",
    "        return y_pred\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset - v2.csv')\n",
    "df_tr = df.loc[df.label=='modeling'] # for crossvalidation and hyperparameter tuning\n",
    "df_test = df.loc[df.label=='oot']\n",
    "model_obj = Default_Prediction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 model trainaing begins (4661, 62)\n",
      "0 data preprocessing starts (4661, 62)\n",
      "1 dropping unwanted column (4661, 47)\n",
      "2 add null columns (4661, 51)\n",
      "3 lower-case cat columns (4661, 51)\n",
      "4 dropping null rows (4510, 51)\n",
      "5 data_imputation (4510, 51)\n",
      "6 remove outliers (4487, 51)\n",
      "7 data_encoding (4487, 60)\n"
     ]
    }
   ],
   "source": [
    "model_obj.model_training(df_tr.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data preprocessing starts (1585, 62)\n",
      "1 dropping unwanted column (1585, 47)\n",
      "2 add null columns (1585, 51)\n",
      "3 lower-case cat columns (1585, 51)\n",
      "5 data imputation (1585, 51)\n",
      "7 data_encoding (1585, 60)\n"
     ]
    }
   ],
   "source": [
    "df_test['yhat'] = model_obj.model_prediction(df_test.copy())\n",
    "output = df_test[['decision_id', 'yhat']]\n",
    "output.to_excel('output.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
